# -*- coding: utf-8 -*-
"""M23CSA017_M23CSA007_M23CSE017.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JnjVbeNIvp8jvvmLI4BCoi413xIk8CAn

VQ_VAE
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import torch
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
from torchvision.utils import make_grid
import torch.nn.functional as F
import torchvision
from torchvision.transforms import transforms
from torch.utils.data import Dataset
from PIL import Image
from tqdm import tqdm
import torch.nn as nn
import pandas as pd
import numpy as np
import torch.optim as optim
from torchvision.io import read_image
import wandb
import warnings
import numpy as np
import os
import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
warnings.filterwarnings("ignore", message="The default value of the antialias parameter.*")

wandb.init(project="VQ_VAE_DL_5")

class ISICDataset(Dataset):
    def __init__(self, img_dir, annotations_file, transform=True, target_transform=None):
        self.img_labels = pd.read_csv(annotations_file)
        self.img_dir = img_dir
        self.transform = transform
        self.target_transform = target_transform

    def __len__(self):
        return len(self.img_labels)

    def __getitem__(self, idx):
        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0] + ".jpg")

        label = np.argmax(self.img_labels.iloc[idx, 1:].astype(None))
        img = Image.open(img_path)

        test_transform = transforms.Compose([
            transforms.ToTensor(),
        ])

        transform = transforms.Compose([
            transforms.RandomHorizontalFlip(),
            transforms.Resize((256, 256)),
            transforms.RandomVerticalFlip(),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
            transforms.ToTensor(),
            transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),
        ])

        if self.transform:
            image = transform(img)
        else:
            image = test_transform(img)

        if self.target_transform:
            label = self.target_transform(label)
        return image, label


# Define dataset paths
train_dir = "/scratch/m23csa017/DL_Assignment_5/dataset_5/Train_data"
test_dir = "/scratch/m23csa017/DL_Assignment_5/dataset_5/Test/Test_data"
train_labels = "/scratch/m23csa017/DL_Assignment_5/dataset_5/Test/Train_labels.csv"
test_labels = "/scratch/m23csa017/DL_Assignment_5/dataset_5/Test/Test_labels.csv"



train_data = ISICDataset(train_dir, train_labels, transform=True)
test_data = ISICDataset(test_dir, test_labels,transform=False)

batch_size = 32

# # Create data loaders
train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=2)


# Display a sample batch shape
images, labels = next(iter(train_loader))
for image in images:
    print(image.shape)
    break

# Device configuration
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Model parameters
epochs = 500
lr = 0.0002
decay = 0.0001
embedding_size = 512


class ResBlock(nn.Module):
    def __init__(self, in_channels, hidden_channels):
        super().__init__()
        self.block = nn.Sequential(
          nn.ReLU(True),
          nn.Conv2d(in_channels, hidden_channels, kernel_size=3, stride=1, padding=1, bias=False),
          nn.BatchNorm2d(hidden_channels),
          nn.ReLU(True),
          nn.Conv2d(hidden_channels, in_channels, kernel_size=1, stride=1, padding=0, bias=False),
          nn.BatchNorm2d(in_channels)
        )

    def forward(self, x):
        return x + self.block(x)

class VQ(nn.Module):

    def __init__(self, num_embeddings, embedding_size, commitment_cost=0.25):
        super().__init__()
        self.num_embeddings = num_embeddings
        self.embedding_size = embedding_size
        self.commitment_cost = commitment_cost

        self.embedding = nn.Embedding(num_embeddings, embedding_size)
        self.embedding.weight.data.uniform_(-1. / num_embeddings, 1. / num_embeddings)

    def forward(self, x):
        x = x.permute(0, 2, 3, 1).contiguous()  # from BCHW to BHWC
        x_flat = x.view(-1, self.embedding_size)

        w = self.embedding.weight
        distances = torch.sum(x_flat ** 2, dim=1, keepdim=True) + torch.sum(w ** 2, dim=1) - 2 * (x_flat @ w.T)
        indices_flat = torch.argmin(distances, dim=1, keepdim=True)
        quantized_flat = self.embed(indices_flat)

        quantized = quantized_flat.view(x.shape)
        indices = indices_flat.view(*x.shape[:3]).unsqueeze(dim=1)  # BHW to BCHW


        if self.training:
            e_latent_loss = F.mse_loss(quantized.detach(), x)
            q_latent_loss = F.mse_loss(quantized, x.detach())
            loss = q_latent_loss + self.commitment_cost * e_latent_loss

            quantized = x + (quantized - x).detach()
        else:
            loss = 0.

        quantized = quantized.permute(0, 3, 1, 2).contiguous()  # from BHWC to BCHW

        return quantized, indices, loss

    def embed(self, indices):
        quantized = self.embedding(indices)
        return quantized

class VQVAE(nn.Module):

    def __init__(self, in_channels, num_embeddings, embedding_size=32, res_hidden_channels=32, commitment_cost=0.25):
        super().__init__()
        self.in_channels = in_channels
        self.num_embeddings = num_embeddings
        self.embedding_size = embedding_size

        h = embedding_size
        self.encoder = nn.Sequential(
            nn.Conv2d(in_channels, h, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(h),
            nn.ReLU(inplace=True),
            nn.Conv2d(h, h, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(h),
            ResBlock(h, res_hidden_channels),
            ResBlock(h, res_hidden_channels),
            ResBlock(h, res_hidden_channels)
        )

        self.vq = VQ(num_embeddings, embedding_size, commitment_cost)

        self.decoder = nn.Sequential(
            ResBlock(h, res_hidden_channels),
            ResBlock(h, res_hidden_channels),
            ResBlock(h, res_hidden_channels),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(h, h, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(h),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(h, in_channels, kernel_size=4, stride=2, padding=1)
        )

    def forward(self, x):
        z = self.encode(x)
        quantized, indices, vq_loss = self.quantize(z)
        x_recon = self.decode(quantized)
        return x_recon, quantized, indices, vq_loss

    def encode(self, x):
        z = self.encoder(x)
        return z

    def quantize(self, z):
        quantized, indices, vq_loss = self.vq(z)
        return quantized, indices, vq_loss

    def decode(self, quantized):
        x_recon = self.decoder(quantized)
        return x_recon

    def embed(self, indices):
        return self.vq.embed(indices)

vq_vae = VQVAE(3, 128, embedding_size).to(device)
optimizer = torch.optim.Adam(vq_vae.parameters(), lr=lr, weight_decay=decay)

def generate_samples(images, model):
    with torch.no_grad():
        images = images.to(device)
        x_recon, _, _, _ = model(images)
    return x_recon

fixed_images, _ = next(iter(test_loader))
fixed_grid = make_grid(fixed_images, nrow=10, normalize=True)

loss_recon = []
loss_quan = []

val_loss_recon = []
val_loss_quan = []

j = 0
count = 0

output_dir = "output_vqvae"
os.makedirs(output_dir, exist_ok=True)

for epoch in range(epochs):
    for i, (x, _) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}')):
        x = x.to(device)

        optimizer.zero_grad()
        x_recon, quantized, encoding_indices, vq_loss = vq_vae(x)


        recon_loss = F.mse_loss(x_recon, x)
        loss = vq_loss + recon_loss

        loss_recon.append(recon_loss.item())
        loss_quan.append(vq_loss.item())

        loss.backward()
        optimizer.step()


        if ((i+1) % 100 == 0):
            with torch.set_grad_enabled(False):
                x_val, _ = next(iter(test_loader))
                x_val = x_val.to(device)

                x_recon, quantized, encoding_indices, val_vq_loss = vq_vae(x)

                val_recon_loss = F.mse_loss(x_recon, x)

                val_loss = val_vq_loss + val_recon_loss

                val_loss_recon.append(val_recon_loss.item())
                val_loss_quan.append(val_vq_loss.item())

                wandb.log({"Training Reconstruction Loss": recon_loss.item(), "Training Quantization Loss": vq_loss.item()})
                wandb.log({"Validation Reconstruction Loss": val_recon_loss.item(), "Validation Quantization Loss": val_vq_loss.item()})


                print('[%d/%d][%d/%d]\t loss: %.4f\t val_loss: %.4f\t'
#                   % (epoch + 1, epochs, i + 1, len(train_loader), loss.item(), val_loss.item()))


                if ((epoch+1) % 1 == 0):  # Save images after each epoch
                    with torch.no_grad():
                        fixed_images = fixed_images.to(device)
                        reconstructed_images, _, _, _ = vq_vae(fixed_images)
                        reconstructed_images = reconstructed_images.cpu()

                    grid = make_grid(reconstructed_images, nrow=10, normalize=True)
                    image_name = f"epoch_{epoch+1}_grid.png"
                    image_path = os.path.join(output_dir, image_name)
                    torchvision.utils.save_image(grid, image_path)
                    print(f"Saved: {image_path}")

                j += 1
        count += 1


model_dir = "models"
os.makedirs(model_dir, exist_ok=True)
torch.save(vq_vae.state_dict(), os.path.join(model_dir, '{}_vqvae.pt'.format("ISIC")))
print("Model saved!")

"""PIXEL CNN"""

import os
import torch
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
from torchvision.utils import make_grid
import torch.nn.functional as F
import torchvision
from torchvision.transforms import transforms
from torch.utils.data import Dataset
from PIL import Image
from tqdm import tqdm
import torch.nn as nn
import pandas as pd
import numpy as np
import torch.optim as optim
from torchvision.io import read_image
import wandb
import warnings
import numpy as np
from torch.distributions import Categorical
warnings.filterwarnings("ignore", message="The default value of the antialias parameter.*")

wandb.init(project="vq_vae_pixel_cnn")

device = "cuda" if torch.cuda.is_available() else "cpu"
batch_size = 32

class ISICDataset(Dataset):
    def __init__(self, img_dir, annotations_file, transform=True, target_transform=None):
        self.img_labels = pd.read_csv(annotations_file)
        self.img_dir = img_dir
        self.transform = transform
        self.target_transform = target_transform

    def __len__(self):
        return len(self.img_labels)

    def __getitem__(self, idx):
        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0] + ".jpg")

        label = np.argmax(self.img_labels.iloc[idx, 1:].astype(None))
        img = Image.open(img_path)

        test_transform = transforms.Compose([
            transforms.ToTensor(),
        ])

        transform = transforms.Compose([
            transforms.RandomHorizontalFlip(),
            transforms.Resize((256, 256)),
            transforms.RandomVerticalFlip(),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
            transforms.ToTensor(),
            transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),
        ])

        if self.transform:
            image = transform(img)
        else:
            image = test_transform(img)

        if self.target_transform:
            label = self.target_transform(label)
        return image, label


# Define dataset paths
train_dir = "/scratch/m23csa017/DL_Assignment_5/dataset_5/Train_data"
test_dir = "/scratch/m23csa017/DL_Assignment_5/dataset_5/Test/Test_data"
train_labels = "/scratch/m23csa017/DL_Assignment_5/dataset_5/Test/Train_labels.csv"
test_labels = "/scratch/m23csa017/DL_Assignment_5/dataset_5/Test/Test_labels.csv"


train_data = ISICDataset(train_dir, train_labels, transform=True)
test_data = ISICDataset(test_dir, test_labels,transform=False)


epochs = 500
lr = 0.0002
decay = 0.0001
embedding_size = 512

def convert_dataset(model, dataset, batch_size):
        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)
        all_indices = []
        all_y = []
        model.eval()
        for x, y in dataloader:
            x = x.to(device)
            with torch.no_grad():
                z = model.encode(x)
                quantized, indices, _ = model.quantize(z)
            all_indices.append(indices)
            all_y.append(y)

        indices = torch.cat(all_indices, dim=0)
        y = torch.cat(all_y, dim=0)

        print(indices.shape, y.shape)
        return torch.utils.data.TensorDataset(indices, y)


class ResBlock(nn.Module):
    def __init__(self, in_channels, hidden_channels):
        super().__init__()
        self.block = nn.Sequential(
          nn.ReLU(True),
          nn.Conv2d(in_channels, hidden_channels, kernel_size=3, stride=1, padding=1, bias=False),
          nn.BatchNorm2d(hidden_channels),
          nn.ReLU(True),
          nn.Conv2d(hidden_channels, in_channels, kernel_size=1, stride=1, padding=0, bias=False),
          nn.BatchNorm2d(in_channels)
        )

    def forward(self, x):
        return x + self.block(x)

class VQ(nn.Module):

    def __init__(self, num_embeddings, embedding_size, commitment_cost=0.25):
        super().__init__()
        self.num_embeddings = num_embeddings
        self.embedding_size = embedding_size
        self.commitment_cost = commitment_cost

        self.embedding = nn.Embedding(num_embeddings, embedding_size)
        self.embedding.weight.data.uniform_(-1. / num_embeddings, 1. / num_embeddings)

    def forward(self, x):
        x = x.permute(0, 2, 3, 1).contiguous()  # from BCHW to BHWC
        x_flat = x.view(-1, self.embedding_size)

        w = self.embedding.weight
        distances = torch.sum(x_flat ** 2, dim=1, keepdim=True) + torch.sum(w ** 2, dim=1) - 2 * (x_flat @ w.T)
        indices_flat = torch.argmin(distances, dim=1, keepdim=True)
        quantized_flat = self.embed(indices_flat)

        quantized = quantized_flat.view(x.shape)
        indices = indices_flat.view(*x.shape[:3]).unsqueeze(dim=1)  # BHW to BCHW


        if self.training:
            e_latent_loss = F.mse_loss(quantized.detach(), x)
            q_latent_loss = F.mse_loss(quantized, x.detach())
            loss = q_latent_loss + self.commitment_cost * e_latent_loss

            quantized = x + (quantized - x).detach()
        else:
            loss = 0.

        quantized = quantized.permute(0, 3, 1, 2).contiguous()  # from BHWC to BCHW
        return quantized, indices, loss

    def embed(self, indices):
        quantized = self.embedding(indices)
        return quantized

class VQVAE(nn.Module):

    def __init__(self, in_channels, num_embeddings, embedding_size=32, res_hidden_channels=32, commitment_cost=0.25):
        super().__init__()
        self.in_channels = in_channels
        self.num_embeddings = num_embeddings
        self.embedding_size = embedding_size

        h = embedding_size
        self.encoder = nn.Sequential(
            nn.Conv2d(in_channels, h, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(h),
            nn.ReLU(inplace=True),
            nn.Conv2d(h, h, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(h),
            ResBlock(h, res_hidden_channels),
            ResBlock(h, res_hidden_channels),
            ResBlock(h, res_hidden_channels)
        )

        self.vq = VQ(num_embeddings, embedding_size, commitment_cost)

        self.decoder = nn.Sequential(
            ResBlock(h, res_hidden_channels),
            ResBlock(h, res_hidden_channels),
            ResBlock(h, res_hidden_channels),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(h, h, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(h),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(h, in_channels, kernel_size=4, stride=2, padding=1)
        )

    def forward(self, x):
        z = self.encode(x)
        quantized, indices, vq_loss = self.quantize(z)
        x_recon = self.decode(quantized)
        return x_recon, quantized, indices, vq_loss

    def encode(self, x):
        x = x.float()
        z = self.encoder(x)
        return z

    def quantize(self, z):
        quantized, indices, vq_loss = self.vq(z)
        return quantized, indices, vq_loss

    def decode(self, quantized):
        x_recon = self.decoder(quantized)
        return x_recon

    def embed(self, indices):
        return self.vq.embed(indices)


vq_vae = VQVAE(3, 128, embedding_size).to(device)
vq_vae.load_state_dict(torch.load('models/{}_vqvae.pt'.format("ISIC")))
vq_vae.eval()

z_train_dataset = convert_dataset(vq_vae, train_data, batch_size=batch_size)
z_test_dataset = convert_dataset(vq_vae, test_data, batch_size=batch_size)


class GatedActivation(nn.Module):

        def __init__(self):
            super().__init__()

        def forward(self, x):
            x, y = x.chunk(2, dim=1)
            return torch.tanh(x) * torch.sigmoid(y)


class GatedMaskedConv2d(nn.Module):

    def __init__(self, mask_type, hidden_channels, kernel_size, residual):
        super().__init__()
        self.mask_type = mask_type
        self.residual = residual

        h = hidden_channels

        self.vert_stack = nn.Conv2d(
            h, h * 2, kernel_size=(kernel_size // 2 + 1, kernel_size), stride=1,
            padding=(kernel_size // 2, kernel_size // 2)
        )
        self.vert_to_horiz = nn.Conv2d(h * 2, h * 2, kernel_size=1, stride=1, padding=0)
        self.horiz_stack = nn.Conv2d(h, h * 2, kernel_size=(1, kernel_size // 2 + 1), stride=1,
                                    padding=(0, kernel_size // 2))
        self.horiz_resid = nn.Conv2d(h, h, kernel_size=1, stride=1, padding=0)

        self.gate = GatedActivation()

    def make_causal(self):
        self.vert_stack.weight.data[:, :, -1].zero_()  # mask final row
        self.horiz_stack.weight.data[:, :, :, -1].zero_()  # mask final column

    def forward(self, x_v, x_h):
        if self.mask_type == 'A':
            self.make_causal()

        h_vert = self.vert_stack(x_v)
        h_vert = h_vert[:, :, :x_v.size(-1), :]
        out_v = self.gate(h_vert)

        h_horiz = self.horiz_stack(x_h)
        h_horiz = h_horiz[:, :, :, :x_h.size(-2)]
        v2h = self.vert_to_horiz(h_vert)

        out = self.gate(v2h + h_horiz)
        if self.residual:
            out_h = self.horiz_resid(out) + x_h
        else:
            out_h = self.horiz_resid(out)

        return out_v, out_h

class GatedPixelCNN(nn.Module):

    def __init__(self, in_channels, hidden_channels, output_channels, num_layers):
        super().__init__()

        self.embedding = nn.Embedding(in_channels, hidden_channels)

        self.mask_a = GatedMaskedConv2d('A', hidden_channels, kernel_size=7,
                                        residual=False)
        self.mask_bs = nn.ModuleList([
            GatedMaskedConv2d('B', hidden_channels, kernel_size=3,
                              residual=True) for _ in range(num_layers - 1)])

        self.output_conv = nn.Sequential(
            nn.Conv2d(hidden_channels, output_channels, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(output_channels, in_channels, kernel_size=1, stride=1, padding=0)
        )

    def forward(self, x):
        x = self.embedding(x.view(-1)).view(x.shape + (-1,))  # (B, H, W, C)
        x = x.permute(0, 3, 1, 2)  # (B, C, H, W)

        x_v, x_h = self.mask_a(x, x)
        for mask_b in self.mask_bs:
            x_v, x_h = mask_b(x_v, x_h)

        return self.output_conv(x_h)

    def sample(self, batch_size, shape):
        self.eval()

        x = torch.zeros((batch_size, *shape), dtype=torch.long, device=device)

        with torch.no_grad():
            for i in range(shape[0]):
                for j in range(shape[1]):
                    logits = self.forward(x)
                    dist = torch.distributions.Categorical(logits=logits[:, :, i, j])
                    x[:, i, j] = dist.sample()
        return x

in_channels = vq_vae.num_embeddings
pixel_cnn = GatedPixelCNN(in_channels, 64, 512, 12)
pixel_cnn.to(device)

# optimizer = optim.Adam(pixel_cnn.parameters(), lr=1e-3, weight_decay=decay)
optimizer = optim.AdamW(pixel_cnn.parameters(), lr=1e-4, weight_decay=decay)

dataloader = torch.utils.data.DataLoader(z_train_dataset, batch_size=batch_size, shuffle=True)
val_dataloader = torch.utils.data.DataLoader(z_train_dataset, batch_size=batch_size, shuffle=True)


output_dir = "output__pixelcnn"
os.makedirs(output_dir, exist_ok=True)


# Training loop
train_losses = []
val_losses = []

for epoch in range(epochs):
    print("Epoch:", epoch + 1)
    for i, (x, _) in enumerate(dataloader):  # Remove the label (_)
        x = x.to(device).squeeze(dim=1)

        optimizer.zero_grad()
        out = pixel_cnn(x)
        scores = out.permute(0, 2, 3, 1).reshape(-1, in_channels)

        loss = F.cross_entropy(scores, x.view(-1))

        loss.backward()
        optimizer.step()
        train_losses.append(loss.item())

        # Print loss after processing every batch
        print('[%d/%d][%d/%d]\t loss: %.4f\t' % (epoch + 1, epochs, i + 1, len(dataloader), loss.item()))

        # Calculate validation loss after processing every batch
        with torch.set_grad_enabled(False):
            x, _ = next(iter(val_dataloader))  # Remove the label (_)
            x = x.to(device).squeeze(dim=1)

            out = pixel_cnn(x)
            scores = out.permute(0, 2, 3, 1).reshape(-1, in_channels)
            val_loss = F.cross_entropy(scores, x.view(-1))

            val_losses.append(val_loss.item())
            wandb.log({"Training Loss": loss.item(), "Validation Loss": val_loss.item()})
            print('\t val_loss: %.4f\t' % (val_loss.item()))



class PixelCNNVQVAE(nn.Module):

    def __init__(self, pixelcnn, vqvae, latent_height, latent_width):
        super().__init__()
        self.pixelcnn = pixelcnn
        self.vqvae = vqvae
        self.latent_height = latent_height
        self.latent_width = latent_width

    def sample_prior(self, batch_size, label=None):
        indices = self.pixelcnn.sample(batch_size, (self.latent_height, self.latent_width)).squeeze(dim=1)

        self.vqvae.eval()
        with torch.no_grad():
            quantized = self.vqvae.embed(indices).permute(0, 3, 1, 2)

        return quantized, indices

    def sample(self, batch_size, label=None):
        quantized, indices = self.sample_prior(batch_size)
        with torch.no_grad():
            x_recon = self.vqvae.decode(quantized)
        return x_recon, quantized, indices

torch.save(pixel_cnn.state_dict(), 'models/{}_pixel-cnn.pt'.format("ISIC"))
print("Model saved")

model = PixelCNNVQVAE(pixel_cnn, vq_vae, 128, 128)
print("Model created")
x_recon, quantized, indices = model.sample(10)
print("Sampled")
#DENORM the x_recon
# x_recon = (x_recon + 1) / 2
x_recon = torch.cat([model.sample(8)[0] for _ in range(2)], dim=0)

grid = make_grid(x_recon.cpu(), nrow=8, normalize=True)

image_name = "grid_image.png"

# Define the full path to save the image
image_path = os.path.join(output_dir, image_name)
# Save the grid image
torchvision.utils.save_image(grid, image_path)
# Print confirmation message
print(f"Saved: {image_path}")

"""Inference"""

import os
import torch
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
from torchvision.utils import make_grid
import torch.nn.functional as F
import torchvision
from torchvision.transforms import transforms
from torch.utils.data import Dataset
from PIL import Image
from tqdm import tqdm
import torch.nn as nn
import pandas as pd
import numpy as np
import torch.optim as optim
from torchvision.io import read_image
import wandb
import warnings
import os
import torch
import torch.nn.functional as F
import torch.nn as nn
import torchvision.utils as vutils
import numpy as np
import warnings
warnings.filterwarnings("ignore", message="The default value of the antialias parameter.*")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


class ResBlock(nn.Module):
    def __init__(self, in_channels, hidden_channels):
        super().__init__()
        self.block = nn.Sequential(
          nn.ReLU(True),
          nn.Conv2d(in_channels, hidden_channels, kernel_size=3, stride=1, padding=1, bias=False),
          nn.BatchNorm2d(hidden_channels),
          nn.ReLU(True),
          nn.Conv2d(hidden_channels, in_channels, kernel_size=1, stride=1, padding=0, bias=False),
          nn.BatchNorm2d(in_channels)
        )

    def forward(self, x):
        return x + self.block(x)

class VQ(nn.Module):

    def __init__(self, num_embeddings, embedding_size, commitment_cost=0.25):
        super().__init__()
        self.num_embeddings = num_embeddings
        self.embedding_size = embedding_size
        self.commitment_cost = commitment_cost

        self.embedding = nn.Embedding(num_embeddings, embedding_size)
        self.embedding.weight.data.uniform_(-1. / num_embeddings, 1. / num_embeddings)

    def forward(self, x):
        x = x.permute(0, 2, 3, 1).contiguous()  # from BCHW to BHWC
        x_flat = x.view(-1, self.embedding_size)

        w = self.embedding.weight
        distances = torch.sum(x_flat ** 2, dim=1, keepdim=True) + torch.sum(w ** 2, dim=1) - 2 * (x_flat @ w.T)
        indices_flat = torch.argmin(distances, dim=1, keepdim=True)
        quantized_flat = self.embed(indices_flat)

        quantized = quantized_flat.view(x.shape)
        indices = indices_flat.view(*x.shape[:3]).unsqueeze(dim=1)  # BHW to BCHW


        if self.training:
            e_latent_loss = F.mse_loss(quantized.detach(), x)
            q_latent_loss = F.mse_loss(quantized, x.detach())
            loss = q_latent_loss + self.commitment_cost * e_latent_loss

            quantized = x + (quantized - x).detach()
        else:
            loss = 0.

        quantized = quantized.permute(0, 3, 1, 2).contiguous()  # from BHWC to BCHW
        return quantized, indices, loss

    def embed(self, indices):
        quantized = self.embedding(indices)
        return quantized

class VQVAE(nn.Module):

    def __init__(self, in_channels, num_embeddings, embedding_size=32, res_hidden_channels=32, commitment_cost=0.25):
        super().__init__()
        self.in_channels = in_channels
        self.num_embeddings = num_embeddings
        self.embedding_size = embedding_size

        h = embedding_size
        self.encoder = nn.Sequential(
            nn.Conv2d(in_channels, h, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(h),
            nn.ReLU(inplace=True),
            nn.Conv2d(h, h, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(h),
            ResBlock(h, res_hidden_channels),
            ResBlock(h, res_hidden_channels),
            ResBlock(h, res_hidden_channels)
        )

        self.vq = VQ(num_embeddings, embedding_size, commitment_cost)

        self.decoder = nn.Sequential(
            ResBlock(h, res_hidden_channels),
            ResBlock(h, res_hidden_channels),
            ResBlock(h, res_hidden_channels),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(h, h, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(h),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(h, in_channels, kernel_size=4, stride=2, padding=1)
        )

    def forward(self, x):
        z = self.encode(x)
        quantized, indices, vq_loss = self.quantize(z)
        x_recon = self.decode(quantized)
        return x_recon, quantized, indices, vq_loss

    def encode(self, x):
        x = x.float()
        z = self.encoder(x)
        return z

    def quantize(self, z):
        quantized, indices, vq_loss = self.vq(z)
        return quantized, indices, vq_loss

    def decode(self, quantized):
        x_recon = self.decoder(quantized)
        return x_recon

    def embed(self, indices):
        return self.vq.embed(indices)

num_embeddings = 512
vq_vae = VQVAE(3, 128, num_embeddings).to(device)
vq_vae.load_state_dict(torch.load('models/{}_vqvae.pt'.format("ISIC")))
vq_vae.eval()


class GatedActivation(nn.Module):

        def __init__(self):
            super().__init__()

        def forward(self, x):
            x, y = x.chunk(2, dim=1)
            return torch.tanh(x) * torch.sigmoid(y)


class GatedMaskedConv2d(nn.Module):

    def __init__(self, mask_type, hidden_channels, kernel_size, residual):
        super().__init__()
        self.mask_type = mask_type
        self.residual = residual

        h = hidden_channels

        self.vert_stack = nn.Conv2d(
            h, h * 2, kernel_size=(kernel_size // 2 + 1, kernel_size), stride=1,
            padding=(kernel_size // 2, kernel_size // 2)
        )
        self.vert_to_horiz = nn.Conv2d(h * 2, h * 2, kernel_size=1, stride=1, padding=0)
        self.horiz_stack = nn.Conv2d(h, h * 2, kernel_size=(1, kernel_size // 2 + 1), stride=1,
                                    padding=(0, kernel_size // 2))
        self.horiz_resid = nn.Conv2d(h, h, kernel_size=1, stride=1, padding=0)

        self.gate = GatedActivation()

    def make_causal(self):
        self.vert_stack.weight.data[:, :, -1].zero_()  # mask final row
        self.horiz_stack.weight.data[:, :, :, -1].zero_()  # mask final column

    def forward(self, x_v, x_h):
        if self.mask_type == 'A':
            self.make_causal()

        h_vert = self.vert_stack(x_v)
        h_vert = h_vert[:, :, :x_v.size(-1), :]
        out_v = self.gate(h_vert)

        h_horiz = self.horiz_stack(x_h)
        h_horiz = h_horiz[:, :, :, :x_h.size(-2)]
        v2h = self.vert_to_horiz(h_vert)

        out = self.gate(v2h + h_horiz)
        if self.residual:
            out_h = self.horiz_resid(out) + x_h
        else:
            out_h = self.horiz_resid(out)

        return out_v, out_h

class GatedPixelCNN(nn.Module):

    def __init__(self, in_channels, hidden_channels, output_channels, num_layers):
        super().__init__()

        self.embedding = nn.Embedding(in_channels, hidden_channels)

        self.mask_a = GatedMaskedConv2d('A', hidden_channels, kernel_size=7,
                                        residual=False)
        self.mask_bs = nn.ModuleList([
            GatedMaskedConv2d('B', hidden_channels, kernel_size=3,
                              residual=True) for _ in range(num_layers - 1)])

        self.output_conv = nn.Sequential(
            nn.Conv2d(hidden_channels, output_channels, 1),
            nn.ReLU(inplace=True),
            nn.Conv2d(output_channels, in_channels, kernel_size=1, stride=1, padding=0)
        )

    def forward(self, x):
        x = self.embedding(x.view(-1)).view(x.shape + (-1,))  # (B, H, W, C)
        x = x.permute(0, 3, 1, 2)  # (B, C, H, W)

        x_v, x_h = self.mask_a(x, x)
        for mask_b in self.mask_bs:
            x_v, x_h = mask_b(x_v, x_h)

        return self.output_conv(x_h)

    def sample(self, batch_size, shape):
        self.eval()

        x = torch.zeros((batch_size, *shape), dtype=torch.long, device=device)

        with torch.no_grad():
            for i in range(shape[0]):
                for j in range(shape[1]):
                    logits = self.forward(x)
                    dist = torch.distributions.Categorical(logits=logits[:, :, i, j])
                    x[:, i, j] = dist.sample()
        return x

in_channels = vq_vae.num_embeddings
print(in_channels)
pixel_cnn = GatedPixelCNN(in_channels, 64, 512, 12)
pixel_cnn.to(device)



class PixelCNNVQVAE(nn.Module):

    def __init__(self, pixelcnn, vqvae, latent_height, latent_width):
        super().__init__()
        self.pixelcnn = pixelcnn
        self.vqvae = vqvae
        self.latent_height = latent_height
        self.latent_width = latent_width

    def sample_prior(self, batch_size, label=None):
        indices = self.pixelcnn.sample(batch_size, (self.latent_height, self.latent_width)).squeeze(dim=1)

        self.vqvae.eval()
        with torch.no_grad():
            quantized = self.vqvae.embed(indices).permute(0, 3, 1, 2)

        return quantized, indices

    def sample(self, batch_size, label=None):
        quantized, indices = self.sample_prior(batch_size)
        with torch.no_grad():
            x_recon = self.vqvae.decode(quantized)
        return x_recon, quantized, indices



def inference(vq_vae, pixel_cnn, output_dir, num_samples=5):
    # Create PixelCNNVQVAE model
    model = PixelCNNVQVAE(pixel_cnn, vq_vae, 128, 128)

    # Generate samples
    model.eval()
    with torch.no_grad():
        x_recon, _, _ = model.sample(num_samples)
        #denorm my x_recon
        x_recon = (x_recon + 1) / 2
    # Save the generated samples
    os.makedirs(output_dir, exist_ok=True)
    vutils.save_image(x_recon, os.path.join(output_dir, 'generated_samples.png'), nrow=8, normalize=True)


if __name__ == "__main__":
    print("Infer")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Define and load your VQ-VAE model

    vq_vae.load_state_dict(torch.load('models/ISIC_vqvae.pt'))
    vq_vae.eval()

    # Define and load your PixelCNN model

    pixel_cnn.load_state_dict(torch.load('models/ISIC_pixel-cnn.pt'))
    pixel_cnn.to(device)
    print("Models loaded successfully!")
    # Perform inference
    inference(vq_vae, pixel_cnn, "output__pixelcnn", num_samples=1)